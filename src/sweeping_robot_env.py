import gymnasium as gym
import numpy as np
import pygame
from gymnasium import spaces


class SweepingRobotEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(self, render_mode=None, size=5):
        super().__init__()
        self.size = size
        self.window_size = 512

        # å®šä¹‰çŠ¶æ€ç©ºé—´ï¼Œåœ¨è¿™é‡ŒçŠ¶æ€ç©ºé—´ç­‰åŒäºè§‚æµ‹ç©ºé—´
        self.observation_space = spaces.Tuple(
            (spaces.Discrete(size), spaces.Discrete(size))
        )
        # å®šä¹‰åŠ¨ä½œç©ºé—´
        self.action_space = spaces.Discrete(4)

        # è®¾ç½® agent çš„é»˜è®¤ä½ç½®
        self._agent_default_location = np.array([0, 1])
        # è®°å½•æœºå™¨äººçš„ä½ç½®ï¼Œåœ¨åç»­çš„ reset æ–¹æ³•ä¸­ä¼šè¢«åˆå§‹åŒ–ï¼Œå¹¶åœ¨ step æ–¹æ³•ä¸­æ›´æ–°
        # æ­¤å¤„ä»…ä»…æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œæˆ–è€…æ˜¯å†·å¯åŠ¨çŠ¶æ€
        self._agent_location = np.array([0, 0])
        # åƒåœ¾ã€å……ç”µæ¡©å’Œéšœç¢ç‰©çš„ä½ç½®
        self._trash_location = np.array([3, 4])
        self._charging_station_location = np.array([0, 0])
        self._obstacle_location = np.array([2, 2])

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

        self.window = None
        self.clock = None

        self.icons = {}

        def load_icon(name, file):
            img = pygame.image.load(file)
            img = pygame.transform.scale(
                img,
                (int(self.window_size / self.size), int(self.window_size / self.size)),
            )
            self.icons[name] = img

        load_icon("robot", "icons/robot.png")
        load_icon("trash", "icons/trash.png")
        load_icon("charger", "icons/charger.png")
        load_icon("obstacle", "icons/block.png")

    def _get_obs(self):
        """è¿”å›å½“å‰ agent çš„ä½ç½®

        Returns:
            tuple: å½“å‰ agent çš„ä½ç½®
        """
        return tuple(self._agent_location)

    def _get_info(self):
        """è¿”å›å½“å‰ agent çš„ä½ç½®ä¸åƒåœ¾å’Œå……ç”µæ¡©çš„æ¬§å‡ é‡Œå¾—è·ç¦»ä¿¡æ¯

        Returns:
            list: å½“å‰ agent çš„ä½ç½®ä¸åƒåœ¾å’Œå……ç”µæ¡©çš„æ¬§å‡ é‡Œå¾—è·ç¦»ä¿¡æ¯
        """
        return {
            "distance_to_trash": np.sum(
                np.abs(self._agent_location - self._trash_location)
            ),
            "distance_to_charger": np.sum(
                np.abs(self._agent_location - self._charging_station_location)
            ),
        }

    def reset(self, init_pos=None, seed=None, options=None):
        # å¤„ç† Gymnasium å†…éƒ¨çš„ç§å­ç­‰
        super().reset(seed=seed)

        # 1. ç¡®å®šæ‰€æœ‰å¯èƒ½çš„æœ‰æ•ˆåˆå§‹ä½ç½®
        initial_positions = []
        for r in range(self.size):
            for c in range(self.size):
                pos = np.array([r, c])
                if (
                    not np.array_equal(pos, self._obstacle_location)
                    and not np.array_equal(pos, self._trash_location)
                    and not np.array_equal(pos, self._charging_station_location)
                ):
                    initial_positions.append(pos)

        # 2. ä»æœ‰æ•ˆä½ç½®ä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
        if not initial_positions:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä½ç½®ï¼ˆä¾‹å¦‚ç½‘æ ¼å¤ªå°æˆ–ç‰¹æ®Šç‚¹å¤ªå¤šï¼‰
            # è®¾ç½®ä¸€ä¸ªå¤‡ç”¨/é»˜è®¤çš„åˆå§‹ä½ç½®ï¼Œç¡®ä¿å®ƒä¸æ˜¯å……ç”µæ¡©æˆ–åƒåœ¾
            # (è¿™é‡Œçš„é€»è¾‘å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´ï¼Œä¾‹å¦‚æŠ›å‡ºé”™è¯¯æˆ–é€‰æ‹©ä¸€ä¸ªå°½å¯èƒ½å®‰å…¨çš„ä½ç½®)
            if (
                not np.array_equal(
                    self._agent_default_location, self._charging_station_location
                )
                and not np.array_equal(
                    self._agent_default_location, self._trash_location
                )
                and not np.array_equal(
                    self._agent_default_location, self._obstacle_location
                )
            ):
                self._agent_location = self._agent_default_location
        else:  # å¦‚æœæœ‰æœ‰æ•ˆä½ç½®ï¼Œé‚£ä¹ˆå°±ä»æœ‰æ•ˆä½ç½®ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåˆå§‹ä½ç½®
            # self.np_random æ˜¯ç”± super().reset(seed=seed) è®¾ç½®çš„éšæœºæ•°ç”Ÿæˆå™¨
            # å®ƒä¿è¯äº†å¦‚æœè®¾ç½®äº†ç§å­ï¼Œéšæœºé€‰æ‹©æ˜¯å¯å¤ç°çš„
            idx = self.np_random.integers(0, len(initial_positions))
            self._agent_location = initial_positions[idx]

        observation = self._get_obs()
        info = self._get_info()

        # å¯¹äº RecordVideoï¼Œä¸éœ€è¦åœ¨ reset æ—¶è°ƒç”¨ _render_frame
        if self.render_mode == "human":
            self._render_frame()
        elif self.render_mode == "rgb_array":
            pass

        return observation, info

    def step(self, action):
        reward = 0.0  # é»˜è®¤å¥–åŠ±ï¼Œé¿å…æœªèµ‹å€¼æƒ…å†µ

        direction_vectors = {
            0: np.array([-1, 0]),  # ä¸Š
            1: np.array([1, 0]),  # ä¸‹
            2: np.array([0, -1]),  # å·¦
            3: np.array([0, 1]),  # å³
        }
        previous_location = np.copy(self._agent_location)
        self._agent_location = self._agent_location + direction_vectors[action]
        # é™å®šå…ƒç´ åœ¨ [0, size-1] èŒƒå›´å†…
        self._agent_location = np.clip(self._agent_location, 0, self.size - 1)

        if np.array_equal(self._agent_location, self._obstacle_location):
            self._agent_location = previous_location

        # å®šä¹‰ç»ˆæ­¢ç¬¦å·
        terminated = False
        if np.array_equal(self._agent_location, self._trash_location):
            reward = 5.0
            terminated = True
        elif np.array_equal(self._agent_location, self._charging_station_location):
            reward = 1.0
            terminated = True

        # è®¾ç½®æˆªæ–­ç¬¦å·
        truncated = False
        observation = self._get_obs()
        info = self._get_info()

        # å¯¹äº RecordVideoï¼Œä¸éœ€è¦åœ¨ step æ—¶ä¸»åŠ¨è°ƒç”¨ _render_frame
        # RecordVideo åŒ…è£…å™¨ä¼šåœ¨éœ€è¦æ—¶è°ƒç”¨ env.render()
        if self.render_mode == "human":
            self._render_frame()

        self._last_action = action
        self._last_reward = reward

        return observation, reward, terminated, truncated, info

    def render(self):
        # render æ–¹æ³•ç°åœ¨å¿…é¡»èƒ½å¤„ç† 'rgb_array' æ¨¡å¼å¹¶è¿”å›å›¾åƒ
        if self.render_mode == "rgb_array":
            return self._render_frame()
        elif self.render_mode == "human":
            self._render_frame()  #  å¯¹äº human æ¨¡å¼ï¼Œåªæ¸²æŸ“ä¸è¿”å›
            return None  # Human mode render typically doesn't return
        else:
            super().render()  # æˆ–è€… gym.Env.render(self)

    def _render_frame(self):
        if self.window is None and self.render_mode == "human":
            pygame.init()
            pygame.display.init()
            self.window = pygame.display.set_mode(
                (self.window_size + 300, self.window_size)
            )
            pygame.display.set_caption("Sweeping Robot Env")

        if self.clock is None and self.render_mode == "human":
            self.clock = pygame.time.Clock()

        if pygame.display.get_init() == 0:
            pygame.init()

        # åˆå§‹åŒ–å­—ä½“
        if not hasattr(self, "_info_font"):
            pygame.font.init()
            self._info_font = pygame.font.SysFont("Arial", 20)

        # åˆ›å»º canvas
        canvas = pygame.Surface((self.window_size + 300, self.window_size))
        canvas.fill((255, 255, 255))
        pix_square_size = self.window_size / self.size

        # ç»˜åˆ¶å…ƒç´ å›¾æ ‡
        def draw_icon(name, pos):
            if name in self.icons:
                icon = self.icons[name]
                canvas.blit(
                    icon,
                    (
                        pos[1] * pix_square_size,
                        (self.size - 1 - pos[0]) * pix_square_size,
                    ),
                )

        draw_icon("trash", self._trash_location)
        draw_icon("charger", self._charging_station_location)
        draw_icon("obstacle", self._obstacle_location)
        draw_icon("robot", self._agent_location)

        # ç»˜åˆ¶ç½‘æ ¼
        for x in range(self.size + 1):
            pygame.draw.line(
                canvas,
                (128, 128, 128),
                (0, pix_square_size * x),
                (self.window_size, pix_square_size * x),
                width=1,
            )
            pygame.draw.line(
                canvas,
                (128, 128, 128),
                (pix_square_size * x, 0),
                (pix_square_size * x, self.window_size),
                width=1,
            )

        # ç»˜åˆ¶å³ä¾§ä¿¡æ¯åŒºåŸŸèƒŒæ™¯
        pygame.draw.rect(
            canvas,
            (240, 240, 240),
            pygame.Rect(self.window_size, 0, 300, self.window_size),
        )

        # æ˜¾ç¤º agent çŠ¶æ€ä¿¡æ¯
        info_lines = []
        if hasattr(self, "_last_action"):
            action_name = ["UP", "DOWN", "LEFT", "RIGHT"][self._last_action]
            info_lines.append(f"Action: {action_name}")
        if hasattr(self, "_last_reward"):
            info_lines.append(f"Reward: {self._last_reward:.1f}")
        info_lines.append(
            f"Pos: ({int(self._agent_location[0] + 1)}, {int(self._agent_location[1] + 1)})"
        )

        for i, line in enumerate(info_lines):
            text_surf = self._info_font.render(line, True, (0, 0, 0))
            canvas.blit(text_surf, (self.window_size + 10, 20 + i * 30))

        # æ¸²æŸ“è¾“å‡º
        if self.render_mode == "human":
            self.window.blit(canvas, canvas.get_rect())
            pygame.event.pump()
            pygame.display.update()
            self.clock.tick(self.metadata["render_fps"])
            return None
        elif self.render_mode == "rgb_array":
            return np.transpose(
                np.array(pygame.surfarray.pixels3d(canvas)),
                axes=(1, 0, 2),
            )

    def close(self):
        if self.window is not None:
            pygame.display.quit()
            # pygame.quit() # pygame.quit()ä¼šå¸è½½æ‰€æœ‰pygameæ¨¡å—ï¼Œå¦‚æœå…¶ä»–åœ°æ–¹è¿˜éœ€è¦ pygameï¼Œå¯èƒ½ä¼šå‡ºé—®é¢˜
            self.window = None
        # ç¡®ä¿åœ¨æ‰€æœ‰pygameæ“ä½œå®Œæˆåæ‰è°ƒç”¨pygame.quit()
        # å¯¹äº RecordVideo, åªè¦ display æ¨¡å—å…³é—­å³å¯ï¼Œpygame.quit()å¯ä»¥åœ¨ç¨‹åºå®Œå…¨ç»“æŸæ—¶è°ƒç”¨
        if pygame.get_init():  # æ£€æŸ¥ pygame æ˜¯å¦å·²åˆå§‹åŒ–
            pygame.quit()


if __name__ == "__main__":
    import matplotlib.pyplot as plt
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.distributions import Categorical

    # Policy Network å®šä¹‰
    class PolicyNetwork(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(PolicyNetwork, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)

            # åˆå§‹åŒ–æƒé‡
            nn.init.xavier_uniform_(self.fc1.weight)
            nn.init.xavier_uniform_(self.fc2.weight)
            nn.init.xavier_uniform_(self.fc3.weight)

        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            # æ·»åŠ æ•°å€¼ç¨³å®šæ€§
            x = torch.clamp(x, min=-10, max=10)  # é˜²æ­¢è¿‡å¤§çš„logits
            return F.softmax(x, dim=-1)

    def state_to_tensor(state, size):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºone-hotç¼–ç çš„å¼ é‡"""
        row, col = state
        state_vector = torch.zeros(size * size)
        state_vector[row * size + col] = 1
        return state_vector

    def compute_returns(rewards, gamma):
        """è®¡ç®—æŠ˜æ‰£å›æŠ¥"""
        returns = []
        running_return = 0
        for r in reversed(rewards):
            running_return = r + gamma * running_return
            returns.insert(0, running_return)
        return torch.tensor(returns, dtype=torch.float32)

    def compute_ema(data, alpha=0.9):
        """è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)"""
        ema = []
        if len(data) > 0:
            ema.append(data[0])
            for i in range(1, len(data)):
                ema.append(alpha * ema[-1] + (1 - alpha) * data[i])
        return ema

    def plot_training_results(
        episode_rewards, episode_lengths, save_path="training_results.png"
    ):
        """ç»˜åˆ¶è®­ç»ƒç»“æœå›¾è¡¨"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

        # è®¡ç®—EMA
        ema_rewards = compute_ema(episode_rewards, alpha=0.99)
        ema_lengths = compute_ema(episode_lengths, alpha=0.99)

        # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
        episodes = list(range(len(episode_rewards)))
        ax1.plot(episodes, episode_rewards, "b-", alpha=0.3, label="Reward")
        ax1.plot(episodes, ema_rewards, "r-", linewidth=2, label="EMA Reward (Î±=0.99)")
        ax1.set_xlabel("Episode")
        ax1.set_ylabel("Total Reward")
        ax1.set_title("The change of reward")
        ax1.grid(True, alpha=0.3)
        ax1.legend()

        # ç»˜åˆ¶episodeé•¿åº¦æ›²çº¿
        ax2.plot(episodes, episode_lengths, "g-", alpha=0.3, label="Step")
        ax2.plot(
            episodes, ema_lengths, "orange", linewidth=2, label="EMA Step (Î±=0.99)"
        )
        ax2.set_xlabel("Episode")
        ax2.set_ylabel("Episode Step")
        ax2.set_title("Episode Length")
        ax2.grid(True, alpha=0.3)
        ax2.legend()

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\nè®­ç»ƒç»Ÿè®¡ä¿¡æ¯:")
        print(f"æœ€é«˜å¥–åŠ±: {max(episode_rewards):.2f}")
        print(f"æœ€å100ä¸ªepisodeå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-100:]):.2f}")
        print(f"æœ€å100ä¸ªepisodeå¹³å‡æ­¥æ•°: {np.mean(episode_lengths[-100:]):.2f}")

    def visualize_policy(env, policy_net, save_path="policy_visualization.png"):
        """å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç­–ç•¥"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 10))

        # åŠ¨ä½œç®­å¤´çš„æ–¹å‘
        action_arrows = {
            0: (0, 0.4),  # ä¸Š
            1: (0, -0.4),  # ä¸‹
            2: (-0.4, 0),  # å·¦
            3: (0.4, 0),  # å³
        }

        # ç»˜åˆ¶ç½‘æ ¼
        for i in range(env.size + 1):
            ax.axhline(y=i, color="gray", linewidth=0.5)
            ax.axvline(x=i, color="gray", linewidth=0.5)

        # ä¸ºæ¯ä¸ªçŠ¶æ€ç»˜åˆ¶æœ€ä¼˜åŠ¨ä½œ
        for row in range(env.size):
            for col in range(env.size):
                pos = np.array([row, col])

                # ç‰¹æ®Šä½ç½®æ ‡è®°
                if np.array_equal(pos, env._trash_location):
                    ax.text(
                        col + 0.5,
                        env.size - row - 0.5,
                        "ğŸ—‘ï¸",
                        fontsize=30,
                        ha="center",
                        va="center",
                    )
                    continue
                elif np.array_equal(pos, env._charging_station_location):
                    ax.text(
                        col + 0.5,
                        env.size - row - 0.5,
                        "ğŸ”Œ",
                        fontsize=30,
                        ha="center",
                        va="center",
                    )
                    continue
                elif np.array_equal(pos, env._obstacle_location):
                    ax.add_patch(
                        plt.Rectangle(
                            (col, env.size - row - 1), 1, 1, facecolor="gray", alpha=0.5
                        )
                    )
                    ax.text(
                        col + 0.5,
                        env.size - row - 0.5,
                        "ğŸš«",
                        fontsize=20,
                        ha="center",
                        va="center",
                    )
                    continue

                # è·å–è¯¥çŠ¶æ€çš„åŠ¨ä½œæ¦‚ç‡
                state_tensor = state_to_tensor((row, col), env.size).unsqueeze(0)
                with torch.no_grad():
                    action_probs = policy_net(state_tensor).squeeze().numpy()

                # ç»˜åˆ¶åŠ¨ä½œç®­å¤´ï¼ˆæ ¹æ®æ¦‚ç‡è°ƒæ•´é€æ˜åº¦ï¼‰
                for action, prob in enumerate(action_probs):
                    if prob > 0.1:  # åªæ˜¾ç¤ºæ¦‚ç‡å¤§äº0.1çš„åŠ¨ä½œ
                        dx, dy = action_arrows[action]
                        ax.arrow(
                            col + 0.5,
                            env.size - row - 0.5,
                            dx * prob,
                            dy * prob,
                            head_width=0.1,
                            head_length=0.1,
                            fc="blue",
                            alpha=prob,
                        )

        ax.set_xlim(0, env.size)
        ax.set_ylim(0, env.size)
        ax.set_aspect("equal")
        ax.set_title("å­¦ä¹ åˆ°çš„ç­–ç•¥å¯è§†åŒ–\n(ç®­å¤´æ–¹å‘è¡¨ç¤ºåŠ¨ä½œï¼Œé€æ˜åº¦è¡¨ç¤ºæ¦‚ç‡)")
        ax.set_xlabel("åˆ—")
        ax.set_ylabel("è¡Œ")

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

    def train_policy_gradient(env, policy_net, optimizer, episodes, gamma=0.99):
        """ä½¿ç”¨REINFORCEç®—æ³•è®­ç»ƒç­–ç•¥ç½‘ç»œ"""
        episode_rewards = []
        episode_lengths = []

        print("ç¯å¢ƒä¿¡æ¯:")
        print(f"- åƒåœ¾ä½ç½®: {env._trash_location}")
        print(f"- å……ç”µç«™ä½ç½®: {env._charging_station_location}")
        print(f"- éšœç¢ç‰©ä½ç½®: {env._obstacle_location}\n")

        for episode in range(episodes):
            # æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®
            states = []
            actions = []
            rewards = []

            obs, _ = env.reset()
            done = False
            total_reward = 0
            steps = 0

            while not done and steps < 100:  # æ·»åŠ æœ€å¤§æ­¥æ•°é™åˆ¶
                # å°†çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡
                state_tensor = state_to_tensor(obs, env.size).unsqueeze(0)

                # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
                action_probs = policy_net(state_tensor)

                # é‡‡æ ·åŠ¨ä½œ
                m = Categorical(action_probs)
                action = m.sample()

                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, terminated, truncated, _ = env.step(action.item())

                # è®°å½•æ•°æ®
                states.append(state_tensor)
                actions.append(action)
                rewards.append(reward)

                obs = next_obs
                total_reward += reward
                steps += 1
                done = terminated or truncated

            episode_rewards.append(total_reward)
            episode_lengths.append(steps)

            # è®¡ç®—å›æŠ¥
            returns = compute_returns(rewards, gamma)

            # æ ‡å‡†åŒ–å›æŠ¥ï¼ˆåªæœ‰å½“æœ‰å¤šä¸ªæ­¥éª¤æ—¶æ‰æ ‡å‡†åŒ–ï¼‰
            if len(returns) > 1:
                returns = (returns - returns.mean()) / (returns.std() + 1e-8)
            else:
                # å¦‚æœåªæœ‰ä¸€æ­¥ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
                returns = returns

            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°ç½‘ç»œ
            policy_loss = []
            entropy_bonus = 0
            for state, action, G in zip(states, actions, returns):
                action_probs = policy_net(state)
                m = Categorical(action_probs)
                policy_loss.append(-m.log_prob(action) * G)
                # æ·»åŠ ç†µæ­£åˆ™åŒ–ï¼Œé¼“åŠ±æ¢ç´¢
                entropy_bonus += m.entropy() * 0.01

            # æ›´æ–°ç­–ç•¥ç½‘ç»œ
            optimizer.zero_grad()
            loss = (
                torch.stack(policy_loss).sum() - entropy_bonus
            )  # å‡å»ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰

            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
            if torch.isnan(loss):
                print(f"è­¦å‘Šï¼šEpisode {episode} å‡ºç°NaNæŸå¤±ï¼Œè·³è¿‡æ›´æ–°")
                continue

            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)

            optimizer.step()

            # æ£€æŸ¥ç½‘ç»œå‚æ•°æ˜¯å¦åŒ…å«NaN
            for param in policy_net.parameters():
                if torch.isnan(param).any():
                    print(f"é”™è¯¯ï¼šEpisode {episode} åç½‘ç»œå‚æ•°åŒ…å«NaNï¼Œåœæ­¢è®­ç»ƒ")
                    return episode_rewards, episode_lengths

            # æ‰“å°è¿›åº¦
            if episode % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                print(
                    f"Episode {episode}, Average Reward (last 100): {avg_reward:.2f}, "
                    f"Average Length: {avg_length:.1f}, Current Reward: {total_reward}"
                )

        return episode_rewards, episode_lengths

    # åˆ›å»ºç¯å¢ƒ
    env = SweepingRobotEnv(render_mode="rgb_array", size=5)

    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    input_size = env.size * env.size  # one-hotç¼–ç çš„çŠ¶æ€å¤§å°
    hidden_size = 128
    output_size = env.action_space.n

    policy_net = PolicyNetwork(input_size, hidden_size, output_size)
    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)  # é™ä½å­¦ä¹ ç‡

    # è®­ç»ƒå‚æ•°
    episodes = 20000
    gamma = 0.99

    print("å¼€å§‹è®­ç»ƒPolicy Gradient...")
    episode_rewards, episode_lengths = train_policy_gradient(
        env, policy_net, optimizer, episodes, gamma
    )

    # ç»˜åˆ¶è®­ç»ƒç»“æœ
    print("\nç»˜åˆ¶è®­ç»ƒç»“æœ...")
    plot_training_results(episode_rewards, episode_lengths)

    # å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç­–ç•¥
    print("\nå¯è§†åŒ–å­¦ä¹ åˆ°çš„ç­–ç•¥...")
    visualize_policy(env, policy_net)

    # æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥
    print("\næµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥...")
    test_episodes = 10
    test_rewards = []

    for i in range(test_episodes):
        obs, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0

        while not done and steps < 50:  # é™åˆ¶æœ€å¤§æ­¥æ•°é˜²æ­¢æ­»å¾ªç¯
            state_tensor = state_to_tensor(obs, env.size).unsqueeze(0)
            with torch.no_grad():
                action_probs = policy_net(state_tensor)
                action = torch.argmax(action_probs, dim=1).item()

            obs, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward
            done = terminated or truncated
            steps += 1

        test_rewards.append(total_reward)
        print(f"æµ‹è¯•Episode {i + 1}: æ€»å¥–åŠ± = {total_reward}, æ­¥æ•° = {steps}")

    print(f"\næµ‹è¯•å¹³å‡å¥–åŠ±: {np.mean(test_rewards):.2f}")

    # ä¿å­˜æ¨¡å‹
    torch.save(policy_net.state_dict(), "policy_gradient_model.pth")
    print("æ¨¡å‹å·²ä¿å­˜ä¸º 'policy_gradient_model.pth'")

    env.close()
