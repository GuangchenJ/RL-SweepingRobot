import numpy as np

# ç½‘æ ¼ä¸–ç•Œå‚æ•°
GRID_SIZE = 5
ACTIONS = ["â¬†ï¸", "â¬‡ï¸", "â¬…ï¸", "â¡ï¸"]
ACTION_DELTAS = {"â¬†ï¸": (-1, 0), "â¬‡ï¸": (1, 0), "â¬…ï¸": (0, -1), "â¡ï¸": (0, 1)}
GAMMA = 0.8  # æŠ˜æ‰£å› å­
THETA = 1e-5  # æ”¶æ•›é˜ˆå€¼
REWARD = 0  # æ¯æ­¥ç§»åŠ¨çš„å¥–åŠ±

# ç›®æ ‡çŠ¶æ€å’Œéšœç¢ç‰©
GOAL_STATE = (1, 4)
BATTERY = (4, 0)
OBSTACLE = (2, 2)

# åˆå§‹åŒ–çŠ¶æ€å€¼
V = np.zeros((GRID_SIZE, GRID_SIZE))
V[GOAL_STATE] = 3 / 0.8
V[OBSTACLE] = -10 / 0.8
V[BATTERY] = 1 / 0.8


# å€¼è¿­ä»£ç®—æ³•
def value_iteration():
    while True:
        delta = 0
        new_V = np.copy(V)

        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                state = (i, j)
                if state == GOAL_STATE or state == OBSTACLE or state == BATTERY:
                    continue

                # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
                q_values = []
                for action in ACTIONS:
                    di, dj = ACTION_DELTAS[action]
                    next_i, next_j = i + di, j + dj

                    # æ£€æŸ¥æ˜¯å¦è¶Šç•Œæˆ–é‡åˆ°éšœç¢ç‰©
                    if (
                        (0 <= next_i < GRID_SIZE)
                        and (0 <= next_j < GRID_SIZE)
                        and (next_i, next_j) != OBSTACLE
                    ):
                        next_state = (next_i, next_j)
                        q_value = REWARD + GAMMA * V[next_state]
                    else:
                        # å¦‚æœè¶Šç•Œæˆ–é‡åˆ°éšœç¢ç‰©ï¼Œç•™åœ¨åŸçŠ¶æ€
                        q_value = REWARD + GAMMA * V[i, j]
                    q_values.append(q_value)

                # é€‰æ‹©æœ€å¤§çš„Qå€¼ä½œä¸ºæ–°çŠ¶æ€å€¼
                new_V[i, j] = max(q_values)
                delta = max(delta, abs(new_V[i, j] - V[i, j]))

        V[:] = new_V
        print(V)
        if delta < THETA:
            break


# æå–æœ€ä¼˜ç­–ç•¥
def extract_policy(V):
    policy = np.full((GRID_SIZE, GRID_SIZE), "", dtype=object)
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE):
            state = (i, j)
            if state == GOAL_STATE:
                policy[i, j] = "ğŸ—‘ï¸"
                continue
            elif state == OBSTACLE:
                policy[i, j] = "â™¿ï¸"
                continue
            elif state == BATTERY:
                policy[i, j] = "ğŸ”‹"
                continue

            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼ï¼Œé€‰æ‹©æœ€å¤§çš„
            max_q = float("-inf")
            best_action = None
            for action in ACTIONS:
                di, dj = ACTION_DELTAS[action]
                next_i, next_j = i + di, j + dj
                if (
                    (0 <= next_i < GRID_SIZE)
                    and (0 <= next_j < GRID_SIZE)
                    and (next_i, next_j) != OBSTACLE
                ):
                    q_value = REWARD + GAMMA * V[next_i, next_j]
                else:
                    q_value = REWARD + GAMMA * V[i, j]
                if q_value > max_q:
                    max_q = q_value
                    best_action = action
            policy[i, j] = best_action
    return policy


# è¿è¡Œå€¼è¿­ä»£
value_iteration()

# è·å–æœ€ä¼˜ç­–ç•¥
policy = extract_policy(V)

V[GOAL_STATE] = 0
V[OBSTACLE] = 0
V[BATTERY] = 0

# æ‰“å°æœ€ä¼˜çŠ¶æ€å€¼
print("æœ€ä¼˜çŠ¶æ€å€¼:")
print(np.round(V, 3))

# æ‰“å°æœ€ä¼˜ç­–ç•¥
print("\næœ€ä¼˜ç­–ç•¥:")
for i in range(GRID_SIZE):
    row = []
    for j in range(GRID_SIZE):
        row.append(policy[i, j])
    print(row)
