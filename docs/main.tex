\documentclass[citestyle=gb7714-2015, bibstyle=gb7714-2015,lang=cn,14pt,scheme=chinese]{elegantbook}

\title{深度学习模型的学习动态}
\subtitle{xxxxx}

\author{姜广琛}
\institute{西北工业大学}
\date{2025/04/17}
\version{0.1}
% \bioinfo{自定义}{信息}

% \extrainfo{注意：本模板自 2023 年 1 月 1 日开始，不再更新和维护！}

\setcounter{tocdepth}{3}

\logo{logo-blue.png}
\cover{cover.jpg}

% 本文档命令
\usepackage{minted}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{subcaption}


% 修改标题页的橙色带
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % 参考文献，不要删除
\ExecuteBibliographyOptions{sorting=ynt}

\begin{document}

% \maketitle
\frontmatter

\tableofcontents

\mainmatter%

\chapter{环境设置}

\section{扫地机器人（Sweeping Robot）}

\input{ch1-env/sweep_robot_env.tex}

\section{多开关匹配}

\input{ch1-env/multi_switch_env.tex}

\section{Double Mountain Car}

\input{ch1-env/double_mountain_car_env.tex}

\chapter{实验过程及结果}

在这一部分展示强化学习算法的实现。

\section{SARAS}

\input{ch2/saras.tex}

\section{REINFORCE 策略梯度算法}

\input{ch2/REINFORCE.tex}

\section{\(Q\)-learning 算法}

\input{ch2/q-learning.tex}

\section{PPO}

本实验旨在验证 Proximal Policy Optimization（PPO）算法在多智能体协作环境——Double Mountain Car 环境中的表现。Double Mountain Car 中的两个小车需要独立控制并协作以达到各自目标位置。
本实验拟通过训练 PPO 智能体，探索其在多维动作空间下的学习效果和稳定性。

本实验基于 \href{https://github.com/DLR-RM/stable-baselines3}{Stable Baselines3} 中 PPO 实现算法实现，结合自定义的 Double Mountain Car 环境进行训练。该环境状态空间为四维向量，包含两个小车的位置与速度；动作空间为两个离散动作的组合（每个小车 \(3\) 个动作），共计 \(9\) 个联合动作。为了简化训练，使用了动作封装器将 MultiDiscrete 动作空间映射为单一离散动作空间。

具体代码实现参考附录~\ref{sec:ppo}。

\subsection{实验结果}

训练过程中，智能体不断学习如何通过两个小车的协作调整动作以最大化累计奖励。奖励设计包含每步 \(-1\) 的基础惩罚及基于两个小车位置变化的稠密奖励，促使智能体更快向目标位置移动。采用状态和奖励归一化策略，提高训练的收敛速度和稳定性。
训练监控数据显示，智能体奖励曲线整体呈上升趋势，说明 PPO 有效学习了该多智能体任务。成功率（达到目标的回合比例）也随训练进展显著提高。
如图~\ref{fig:ppo} 所示，基于监控文件生成的奖励、回合长度和成功率曲线均表现出较好的训练收敛性。奖励曲线的指数加权移动平均（EMA）清晰反映出训练的稳定提升。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/double_mountain_car/episode_length.png}
    \par
    \centering (a) 每轮步数
    \par
    \includegraphics[width=0.7\linewidth]{figure/double_mountain_car/reward_curve.png}
    \par
    \centering (b) 收益曲线
    \par
    \includegraphics[width=0.7\linewidth]{figure/double_mountain_car/success_rate.png}
    \par
    \centering (c) 成功率
    \caption{Double Mountain Car 的训练结果曲线}
    \label{fig:ppo}
\end{figure}

本实验成功实现了基于 PPO 算法的 Double Mountain Car 问题训练，验证了 PPO 在多智能体离散动作空间中的学习能力。训练结果表明智能体能够有效协作，达成双车目标任务。

\subsection{参数设置}

本实验中的参数设置如表~\ref{tab:ppo-double-mountain-car-params} 所示：

\begin{table}[htbp]
\centering
\caption{PPO 算法在 Double Mountain Car 环境中的实验参数设置}
\label{tab:ppo-double-mountain-car-params}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{参数名称} & \textbf{含义} & \textbf{设置值} \\
\midrule
环境    & 训练使用的环境 & Double Mountain Car \\
状态空间维度    & 环境观测的特征数 & 4（两个小车的位置和速度） \\
动作空间维度    & 联合动作数量     & 9（两个小车各 3 个动作组合） \\
最大回合步数    & 每回合最大交互步数 & 400 \\
训练总步数  & PPO 训练总采样步数 & 1,000,000 \\
学习率  & 优化器初始学习率  & 0.0003 \\
折扣因子 \(\gamma\) & 奖励折扣系数      & 0.99 \\
批次大小    & PPO 每次更新批量大小 & 512 \\
时间步长 (n\_steps) & 每次 PPO 更新采样步数 & 1024 \\
剪切范围 (clip\_range)  & PPO 重要性采样裁剪范围 & 0.2 \\
奖励稠密化权重  & 位置变化奖励加权  & 2.0 \\
\bottomrule
\end{tabular}
\end{table}

\nocite{*}

\printbibliography[heading=bibintoc, title=\ebibname]
\appendix

\chapter{appendix}

\section{扫地机器人环境代码}\label{sec:sweeping-robot-env}

\input{appendix/sweeping-robot-env.tex}

\section{多开关匹配环境代码}\label{sec:multi-switch-env}

\input{appendix/multi-switch-env.tex}

\section{Double Mountain Car 环境代码}\label{sec:double-mountain-car-env}

\input{appendix/double-mountain-car-env.tex}

\section{针对扫地机器人环境的 SARAS 代码}\label{sec:saras}

\input{appendix/saras.tex}

\section{针对扫地机器人环境的 REINFORCE 策略梯度算法代码}\label{sec:REINFORCE}

\input{appendix/REINFORCE.tex}

\section{针对多开关匹配环境的 \(Q\)-learning 算法代码}\label{sec:q-learning}

\input{appendix/q-learning.tex}

\section{针对 Double Mountain Car 环境的 PPO 算法代码}\label{sec:ppo}

\input{appendix/ppo.tex}

\end{document}
